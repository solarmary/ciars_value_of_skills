{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a33c7118-5e4b-498d-860d-2da3fb5abd4c",
   "metadata": {
    "id": "a33c7118-5e4b-498d-860d-2da3fb5abd4c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import phik\n",
    "from phik.report import plot_correlation_matrix\n",
    "from phik import report\n",
    "import os\n",
    "from sklearn import preprocessing\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.utils import shuffle\n",
    "import random\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import json\n",
    "warnings.simplefilter(\"ignore\")\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 12)\n",
    "import sys\n",
    "import argparse\n",
    "import statistics\n",
    "from catboost import  CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import KFold  # k-фолдная валидация\n",
    "import psycopg2\n",
    "import tqdm\n",
    "import plotly.express as px\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3e0dad",
   "metadata": {
    "id": "6c3e0dad"
   },
   "source": [
    "# Пути к файлам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8e455648-91bb-4798-bce8-e187e238fa9c",
   "metadata": {
    "id": "8e455648-91bb-4798-bce8-e187e238fa9c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "global path_to_test,path_to_train,path_to_e_models,path_to_save_validation,path_to_cb_models,path_to_save_e_model_results,path_to_regions\n",
    "\n",
    "path_to_test = './super_dfs'\n",
    "path_to_train = './Bundles2023'\n",
    "#path_to_train = './based_bundles'\n",
    "path_to_e_models = './e_models_i' #модели-джсоны\n",
    "path_to_cb_models = './cb_models_no_us'\n",
    "path_to_save_validation = './results3/results_validation'\n",
    "path_to_regions = './ind_groups_2.csv'\n",
    "path_to_save_e_model_results = './results3/results_new_salary_quantile'\n",
    "# path_to_drop_cols_ad = '/home/admin/skill_value/adek_cols_25f_last.json'\n",
    "# path_to_drop_cols_po = '/home/admin/skill_value/pop_cols_25f_last.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75f63223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30934, 181)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{path_to_train}/based_b_116.csv')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "12833af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bundles = [116, 119, 124, 128, 134, 16, 23, 27, 283, 31, 33, 336, 357, 43,49,52,54, 561, 58, 61, 663, 69, 693, 706, 712, 74, 742, \\\n",
    " 765, 767, 783, 811, 812, 833, 836, 84, 85, 852, 853, 856, 87, 873, 876, 888, 898, 91, 917, 919, 921, 954, 992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee404fad-e17b-45bd-b2e2-dd4e280e129e",
   "metadata": {
    "id": "ee404fad-e17b-45bd-b2e2-dd4e280e129e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Для запуска из .py формата\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     parser = argparse.ArgumentParser(description = 'train')\n",
    "#     parser.add_argument('--n_bundle')\n",
    "#     parser.add_argument('--salary_column')\n",
    "\n",
    "#     args = parser.parse_args(sys.argv[1:])\n",
    "#     n_bundle = args.n_bundle\n",
    "#     salary_column = args.salary_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "765d024e-ecdb-4866-961e-32d4d838f785",
   "metadata": {
    "id": "765d024e-ecdb-4866-961e-32d4d838f785",
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    connection = psycopg2.connect(user=\"RemoteFA\",\n",
    "                                  database=\"fa\",\n",
    "                                  password=\"RFA_127_BT_fandc\",\n",
    "                                  host=\"rc1b-bdye1rzk75u6dgzf.mdb.yandexcloud.net\",\n",
    "\n",
    "                                  port=\"6432\",\n",
    "                                  sslmode = \"allow\")\n",
    "\n",
    "except Exception as error:\n",
    "    print(\"Ошибка при работе с PostgreSQL\", error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "d6f953d3-9bd3-4bb5-a251-d7597fe191df",
   "metadata": {
    "id": "d6f953d3-9bd3-4bb5-a251-d7597fe191df",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_test(df):\n",
    "    train = df\n",
    "    test = pd.DataFrame()\n",
    "    for is_vahta in [0,1]:\n",
    "        for experience_id in [[0], [1], [2,3]]:\n",
    "            for industry_group in [0, 1, 2]:\n",
    "                curr_df = df[(df.is_vahta == is_vahta) & (df.experience_id.isin(experience_id)) \\\n",
    "                             & (df.industry_group == industry_group)]\n",
    "                if curr_df.shape[0] < 10:\n",
    "                    curr_test = pd.DataFrame()\n",
    "\n",
    "                else:\n",
    "                    train_size_middle = int(curr_df.shape[0] * 0.85)\n",
    "                    test_size_middle = curr_df.shape[0] - train_size_middle\n",
    "\n",
    "                    train_size_side = int(curr_df.shape[0] * 0.95)\n",
    "                    test_size_side = curr_df.shape[0] - train_size_side # сумма коэф. на которые умножаются размеры должна быть равна 1.8\n",
    "\n",
    "                    lower_bound = statistics.mode(curr_df['new_salary']) - curr_df['new_salary'].std()\n",
    "                    upper_bound = statistics.mode(curr_df['new_salary']) + curr_df['new_salary'].std()\n",
    "\n",
    "                    middle_group = curr_df[(curr_df.new_salary >= lower_bound) & (curr_df.new_salary <= upper_bound)]\n",
    "                    side_group = curr_df[~((curr_df.new_salary >= lower_bound) & (curr_df.new_salary <= upper_bound))]\n",
    "\n",
    "\n",
    "\n",
    "                    middle_group, side_group = shuffle(middle_group), shuffle(side_group)\n",
    "\n",
    "                    middle_group_test = middle_group.iloc[0:test_size_middle]\n",
    "                    side_group_test = side_group.iloc[0:test_size_side]\n",
    "\n",
    "                    curr_test = pd.concat([middle_group_test, side_group_test])\n",
    "\n",
    "\n",
    "\n",
    "                if len(test) and len(curr_test):\n",
    "                    test = pd.concat([curr_test, test])\n",
    "                elif len(curr_test):\n",
    "                    test = curr_test\n",
    "\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "d5dcbe42-6b73-4ec8-984f-bb8aa215ef60",
   "metadata": {
    "id": "d5dcbe42-6b73-4ec8-984f-bb8aa215ef60",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_date(df):\n",
    "    df['year'] = df['first_pub_date'].apply(lambda x: int(x.year))\n",
    "    df['month'] = df['first_pub_date'].apply(lambda x: int(x.month))\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "2564d5a1-4cdd-40fc-b85b-4e0cf26e0c3f",
   "metadata": {
    "id": "2564d5a1-4cdd-40fc-b85b-4e0cf26e0c3f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# функция, определяющая группу отрасли специализации региона\n",
    "def get_group(region):\n",
    "\n",
    "    groups = [] # группы, к которым принадлежит регион\n",
    "    for industry in industry_dict.items():\n",
    "        if region in industry[1]:\n",
    "            groups.append(industry[0])\n",
    "    return groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "e5d9362f-4f39-44b7-8019-bd78660fb11d",
   "metadata": {
    "id": "e5d9362f-4f39-44b7-8019-bd78660fb11d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Label encoding  для столбца industry group\n",
    "def preprocess_columns(df):\n",
    "    df['industry_group'] = df['industry_group'].apply(lambda x: ', '.join(x))\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    df['industry_group'] = le.fit_transform(df['industry_group'])\n",
    "    try:\n",
    "        if len(le) == 4:\n",
    "            df['industry_group'] = df['industry_group'].replace(0, 1)\n",
    "            df['industry_group'] = df['industry_group'].replace(1, 0)\n",
    "            df['industry_group'] = df['industry_group'].replace(2, 1)\n",
    "            df['industry_group'] = df['industry_group'].replace(3, 2)\n",
    "    except:\n",
    "        ...\n",
    "    # Обратное преобразование, если необходимо\n",
    "    # df['industry_group'] = le.inverse_transform(df['industry_group'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "462d65a0-8021-40d6-b05c-7905d46406ed",
   "metadata": {
    "id": "462d65a0-8021-40d6-b05c-7905d46406ed",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def svyazi(df, sv):\n",
    "    # Создаем список столбцов из датафрейма, начиная с 6-го столбца\n",
    "    cols = list(df)[6:]\n",
    "\n",
    "    # Создаем список идентификаторов столбцов, извлекая их из названий столбцов\n",
    "    cols_id = [i[i.rfind('(')+1:i.rfind('_')] for i in cols]\n",
    "\n",
    "    # Проходимся по всем элементам списка cols\n",
    "    for i in range(len(cols)):\n",
    "        # Проходимся по всем элементам списка, полученного из словаря sv по ключу, соответствующему текущему cols_id[i]\n",
    "        for j in sv[str(cols_id[i])][1]:\n",
    "            # Проверяем, есть ли текущий элемент j в списке cols_id\n",
    "            if str(j) in cols_id:\n",
    "                # Записываем текущий столбец в переменную par\n",
    "                par = cols[i]\n",
    "                # Записываем столбец, соответствующий элементу j, в переменную child\n",
    "                child = cols[cols_id.index(str(j))]\n",
    "                # Применяем функцию lambda к столбцу child, складывая его с парент-столбцом par\n",
    "                # Если результат равен 2, заменяем его на 1, иначе оставляем без изменений\n",
    "                df[child] = (df[par] + df[child]).apply(lambda x: 1 if x == 2 else x)\n",
    "\n",
    "    # Возвращаем измененный датафрейм\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "8095bd58-4896-47b9-9a9c-510988e31143",
   "metadata": {
    "id": "8095bd58-4896-47b9-9a9c-510988e31143",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_df(n_bundle, salary_column, is_test = 0):\n",
    "    # Чтение данных из CSV файла\n",
    "    if is_test:\n",
    "        df = pd.read_csv(f'{path_to_test}/Bundles_{n_bundle}.csv')\n",
    "    else:\n",
    "        df = pd.read_csv(f'{path_to_train}/Bundles_{n_bundle}.csv')\n",
    "\n",
    "    df = df.drop('group_region',axis = 1)\n",
    "    # Удаление строк с отсутствующими значениями и столбца 'region'\n",
    "    df = df.dropna(axis=0).drop(['region'], axis=1)\n",
    "\n",
    "    # Удаление дубликатов по столбцу 'id'\n",
    "    df = df.drop_duplicates(subset=['id'])\n",
    "\n",
    "    # Преобразование столбца 'first_pub_date' в формат даты и времени\n",
    "    df['first_pub_date'] = pd.to_datetime(df['first_pub_date'])\n",
    "\n",
    "    # Получение даты из столбца 'first_pub_date' и удаление столбца 'first_pub_date'\n",
    "    df = get_date(df).drop('first_pub_date', axis=1)\n",
    "\n",
    "    # Отбор строк, где значение столбца 'is_multiple' равно 0\n",
    "    df = df[df['is_multiple'] == 0]\n",
    "\n",
    "    # Отбор строк, где значение столбца 'new_salary' находится в интервале между 5-м и 95-м перцентилями\n",
    "    # df = df[(df['new_salary'] < np.quantile(np.array(df['new_salary']), 0.95)) & (df['new_salary'] > np.quantile(np.array(df['new_salary']), 0.05))]\n",
    "\n",
    "    # Замена значения столбца 'new_salary' на значение из переменной 'salary_column'\n",
    "    df['new_salary'] = df[salary_column]\n",
    "\n",
    "    # Удаление столбцов, содержащих слово 'salary', за исключением столбца 'new_salary'\n",
    "    df = df.drop([x for x in list(df) if 'salary' in x and x != 'new_salary'], axis=1)\n",
    "\n",
    "    # Получение столбцов с наибольшими значениями\n",
    "    # df1 = top_columns(df)\n",
    "    indexes = df.id\n",
    "    # Определяем базовые столбцы\n",
    "    base_columns = ['year', 'new_salary', 'is_vahta', 'experience_id', 'region_name', 'industry_group']\n",
    "\n",
    "    # Определяем столбцы с навыками\n",
    "    skills_columns = list(df)[list(df).index('year')+1:-1]\n",
    "    skills_columns = [x for x in skills_columns if x != 'new_salary' and x != 'group_region' and x != 'region_name']\n",
    "\n",
    "    # Определяем регионы для студентов\n",
    "    student_regions = ['Город федерального значения Москва', 'Город федерального значения Санкт-Петербург', 'Республика Татарстан', 'Ростовская область', 'Свердловская область', 'Краснодарский край', 'Республика Башкортостан', 'Новосибирская область', 'Самарская область', 'Челябинская область', 'Нижегородская область']\n",
    "\n",
    "    # Определяем сельскохозяйственные регионы\n",
    "    selhoz_regions = ['Краснодарский край', 'Ростовская область', 'Белгородская область', 'Республика Татарстан', 'Воронежская область',\n",
    "                      'Ставропольский край', 'Республика Башкортостан', 'Алтайский край', 'Волгоградская область', 'Тамбовская область',\n",
    "                      'Челябинская область', 'Саратовская область', 'Московская область', 'Курская область', 'Оренбургская область']\n",
    "    selhoz_regions = list(set(selhoz_regions) - set(student_regions))\n",
    "\n",
    "    prom_regions = list(set(df['region_name']) - set(student_regions) - set(selhoz_regions))\n",
    "\n",
    "    global industry_dict\n",
    "\n",
    "    industry_dict = {'промышленность':prom_regions,\n",
    "                     'сельское хозяйство':selhoz_regions,\n",
    "                     'образование':student_regions}\n",
    "\n",
    "\n",
    "    df['industry_group'] = df['region_name'].apply(get_group)\n",
    "    df = preprocess_columns(df)\n",
    "\n",
    "    df = df[base_columns + skills_columns]\n",
    "    df = df.drop('month',axis = 1, errors = 'ignore')\n",
    "    #tree = json.load(open(f'{path_to_train}/bundles_jsones/Bundles_{n_bundle}.json'))\n",
    "    # tree = json.load(open(f'{path_to_train}/jsones/Bundles_{n_bundle}.json'))\n",
    "    tree = json.load(open(f'./Bundles2023/jsones_new/Bundles_{n_bundle}.json'))\n",
    "\n",
    "    df= svyazi(df,tree)\n",
    "\n",
    "    df = df[df['year'] > 2000]\n",
    "\n",
    "    df['experience_id'] = df['experience_id'].apply(lambda x: x if x != 3 else 2)\n",
    "\n",
    "    return df,indexes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "ef941e0d-b183-4d46-ae53-d333d3db2929",
   "metadata": {
    "id": "ef941e0d-b183-4d46-ae53-d333d3db2929",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def year_coeff_for_base_skills(df):\n",
    "    # Поправка заказчика №8\n",
    "    to_skip = ['id', 'salary_from_rub', 'source_site', 'vacancies_type_id', 'is_vahta', 'is_shift', 'is_distance', 'is_parttime', 'v3_region_index', 'is_multiple', 'description_hash2', 'experience_id', 'is_junior', 'is_senior', 'part', 'group_region', 'new_salary', 'region_name', 'region']\n",
    "\n",
    "    # Удаляем столбцы, указанные в to_skip\n",
    "    df = df[[name for name in list(df) if not name in to_skip]]\n",
    "\n",
    "    # Сортируем по столбцу 'year'\n",
    "    df = df.sort_values(by=['year'])\n",
    "\n",
    "    # Получаем уникальные значения столбца 'year' и сортируем их\n",
    "    dates = sorted(set(list(df['year'])))\n",
    "\n",
    "    # Группируем данные по столбцу 'year' и считаем количество записей в каждой группе\n",
    "    df_counts = df.groupby(by='year').count()\n",
    "\n",
    "    # Группируем данные по столбцу 'year' и суммируем значения в каждой группе\n",
    "    df_sums = df.groupby(by='year').sum()\n",
    "\n",
    "    # Вычисляем среднее значение для каждого столбца\n",
    "    df = df_sums / df_counts\n",
    "\n",
    "    # Преобразуем данные в одномерный массив\n",
    "    all_vals = np.array(df.values.reshape(1, df.shape[0] * df.shape[1])[0])\n",
    "\n",
    "    # Создаем пустой список для хранения названий хороших навыков\n",
    "    good_skills = []\n",
    "\n",
    "    # Проверяем каждый столбец на упорядоченность значений\n",
    "    for name in list(df):\n",
    "        arr = list(df[name])\n",
    "        for i in range(len(arr) - 1):\n",
    "            if arr[i] > arr[i + 1]:\n",
    "                break\n",
    "        else:\n",
    "            good_skills.append(name)\n",
    "\n",
    "    # Если нет хороших навыков, возвращаем словарь с датами и значениями 1\n",
    "    if not good_skills:\n",
    "        return {dates[i]:1 for i in range(len(dates))}\n",
    "\n",
    "    # Оставляем только столбцы с хорошими навыками\n",
    "    df = df[good_skills]\n",
    "\n",
    "    # Вычисляем среднее значение для каждой строки\n",
    "    df = df.mean(axis=1)\n",
    "\n",
    "    # Задаем значение эпсилон\n",
    "    eps = 1e-7\n",
    "\n",
    "\n",
    "    importance = (all_vals <= np.array(df).mean()).sum() / len(all_vals) #по сути считакм квантель (какой процент от всех наша средняя частота обходит)\n",
    "\n",
    "    need_to_use = 1 - importance\n",
    "    arr = np.array(list(df)) + eps #добавляем очень маленькое число чтобы избежать деление на ноль\n",
    "\n",
    "    nparr = np.array(arr)\n",
    "\n",
    "    k = [arr[i + 1] / arr[i] for i in range(len(arr) - 1) if arr[i] > 0]\n",
    "\n",
    "\n",
    "\n",
    "    # Расчет значения cf\n",
    "    cf = [1 / np.prod(np.array(k[i:])) for i in range(len(k))] + [1]\n",
    "\n",
    "    # Расчет диапазона значений OldRange\n",
    "    OldRange = (max(cf) - min(cf))\n",
    "\n",
    "    # Проверка, если OldRange равен 0\n",
    "    if OldRange == 0:\n",
    "        # Возвращаем словарь с единичными значениями для каждой даты\n",
    "        return {dates[i]: 1 for i in range(len(dates))}\n",
    "\n",
    "    # Расчет диапазона значений NewRange\n",
    "    NewRange = (1 - need_to_use)\n",
    "\n",
    "    # Расчет нового значения NewValue с использованием формулы для масштабирования\n",
    "    # делаем scale от need_to_use до 1, т.к. считаем что хотя бы need_to_use должен быть\n",
    "    NewValue = (((np.array(cf) - min(cf)) * NewRange) / OldRange) + need_to_use\n",
    "\n",
    "    # Возвращаем словарь с новыми значениями для каждой даты\n",
    "    return {dates[i]: NewValue[i] for i in range(len(NewValue))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "68367d23-b93b-4e95-a915-6e81c270a6ff",
   "metadata": {
    "id": "68367d23-b93b-4e95-a915-6e81c270a6ff",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def linearize(input_array, relation_coefficient):\n",
    "    # Create a new array that starts with the first value of the original array\n",
    "    # and increments each subsequent value by the relation coefficient.\n",
    "    transformed_array = np.array([input_array[0] + i*relation_coefficient for i in range(len(input_array))])\n",
    "\n",
    "    # Calculate the scaling factor as the ratio of the sum of the original array to the sum of the transformed array.\n",
    "    scaling_factor = np.sum(input_array) / np.sum(transformed_array)\n",
    "\n",
    "    # Scale the transformed array to make its sum equal to the sum of the original array.\n",
    "    transformed_array = transformed_array * scaling_factor\n",
    "\n",
    "    return transformed_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "97d467be-4426-4aef-ba0d-9557110a8e5d",
   "metadata": {
    "id": "97d467be-4426-4aef-ba0d-9557110a8e5d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#функция чтобы домножать навыки которые в строке = 1 на коэф их года\n",
    "def adjust_row(row):\n",
    "    if str(row['year']) in list(year_coef_dict.keys()):\n",
    "        return row * year_coef_dict[row['year']]\n",
    "    else:\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "05c60e08-aaaa-428f-84d1-27dc7dad50e5",
   "metadata": {
    "id": "05c60e08-aaaa-428f-84d1-27dc7dad50e5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция которая возвращает популярность навыков\n",
    "def make_freq_df(df,experience_id,is_vahta,industry_group,skills_columns,year_coef):\n",
    "    freq_df = df[df['experience_id'].isin(experience_id) &\\\n",
    "                              (df['is_vahta'] == is_vahta) & \\\n",
    "                              (df['industry_group'].isin(industry_group))][['year']+skills_columns]\n",
    "    if year_coef:\n",
    "        freq_df = freq_df.apply(adjust_row, axis=1)\n",
    "    skills_columns = [x for x in skills_columns if x != 'year']\n",
    "    freq_df = freq_df.drop('year',axis =1)\n",
    "    freq_df = freq_df[skills_columns].sum().sort_values(ascending=False)\n",
    "\n",
    "\n",
    "    freq_df = freq_df.reset_index().rename(columns={'index': 'skill', 0: 'frequency'})\n",
    "    return freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0edb080b-0f22-464e-86ab-b9a888186227",
   "metadata": {
    "id": "0edb080b-0f22-464e-86ab-b9a888186227",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Вычисляет коэфы зп для каждого региона\n",
    "\n",
    "def get_region_coefs(df,is_vahta,experience_id,industry_group,sposob):\n",
    "\n",
    "    df = df.groupby('region_name').mean().sort_values(by = 'new_salary').reset_index()\n",
    "\n",
    "    max_s = float(df.iloc[-1]['new_salary'])\n",
    "    min_s = float(df.iloc[0]['new_salary'])\n",
    "    mean_s = float(df.iloc[df.shape[0]//2]['new_salary'])\n",
    "\n",
    "    if sposob == 0:\n",
    "        coefs = list(np.linspace(min_s/mean_s, max_s/mean_s, df.shape[0]))[::-1]\n",
    "\n",
    "    elif sposob == 1:\n",
    "        coefs = list(np.linspace(min_s/mean_s, 1.3, df.shape[0]))[::-1]\n",
    "\n",
    "    elif sposob == 2:\n",
    "        coefs = list(np.linspace(0.8,1.2 , df.shape[0]))[::-1]\n",
    "\n",
    "    else:\n",
    "        coefs = list(np.linspace(min_s/mean_s, max_s/mean_s, df.shape[0]))[::-1]\n",
    "\n",
    "\n",
    "    coef_dict = {}\n",
    "    for i,j in zip(coefs[::-1],df['region_name']):\n",
    "        coef_dict[j] = i\n",
    "\n",
    "\n",
    "    return coef_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "ccac2366-bd99-48d3-a90b-9c8523a0fa5b",
   "metadata": {
    "id": "ccac2366-bd99-48d3-a90b-9c8523a0fa5b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Функция, которая делит навыки на группы\n",
    "\n",
    "def skills_smart_decomposition(df,experience_id,is_vahta,\\\n",
    "                               industry_group,q_base,q_regular,\\\n",
    "                               q_rare,skills_columns,year_coef):\n",
    "\n",
    "    # Начинаем с того, что получаем популярность навыков, отсортированную в порядке убывания\n",
    "    fdf = make_freq_df(df,experience_id,is_vahta,industry_group,skills_columns,year_coef)\n",
    "    fdf = fdf[fdf['frequency'] > 0]\n",
    "\n",
    "    #Если навыков меньше 10, то профессия - брак (зачем? почему?)\n",
    "    if fdf.shape[0] < 10:\n",
    "        return [0,0,0,0]\n",
    "\n",
    "    #Базовые скилы - те, кол-во которых больше q_base квантиля ( 0.9 ) в нашем случае\n",
    "    base_skills = (fdf[fdf['frequency'] > np.quantile(fdf['frequency'],q_base)])\n",
    "\n",
    "    # Эта переменная считает квантиль для ОБЫЧНЫХ навыков таким образом, что\n",
    "    # в подсчете не участвуют базовые навыки. То есть сначала убираем их\n",
    "    # значения, а потом считаем новый квантиль (0.75) в нашем случае\n",
    "    fq = np.quantile(fdf[fdf['frequency'] < np.quantile(fdf['frequency'],\\\n",
    "                                        q_base)]['frequency'], q_regular)\n",
    "\n",
    "    #Обычные - те, которые больше квантиля выше. Синтаксис после & отсекает базовые навыки.\n",
    "    regular_skills = fdf[(fdf['frequency'] > fq) & (fdf['frequency'] < \\\n",
    "                                                    np.quantile(fdf['frequency'],q_base))]\n",
    "\n",
    "\n",
    "    # Не всегда такой подход позволяет разделить навыки на 3-4 части, поэтому если после\n",
    "    # Разделения на базовые навыки возникает ошибка, то я возвращаю только базовые и обычные,\n",
    "    # см. return\n",
    "    try:\n",
    "        #Делаем то же что и для обычных навыков, по аналогии с отсечением базовых\n",
    "        fqq = np.quantile(fdf[(fdf['frequency'] < fq)]['frequency'],q_rare)\n",
    "        rare_skills = fdf[(fdf['frequency'] < fq)&(fdf['frequency'] > fqq)]\n",
    "    except:\n",
    "        # try:\n",
    "        #     make_plot(fdf,str(regular_skills['skill'].iloc[0]),str(regular_skills['skill'].iloc[-1]),'')\n",
    "        # except:\n",
    "        #     ...\n",
    "\n",
    "        return [base_skills,regular_skills,pd.DataFrame(columns = ['skill','frequency']),pd.DataFrame(columns = ['skill','frequency'])]\n",
    "\n",
    "\n",
    "    useless_skills = fdf[(fdf['frequency'] < fqq)]\n",
    "\n",
    "    # try:\n",
    "    #     make_plot(fdf,str(regular_skills['skill'].iloc[0]),str(rare_skills['skill'].iloc[0]),str(rare_skills['skill'].iloc[-1]))\n",
    "    # except:\n",
    "    #     ...\n",
    "\n",
    "    return [base_skills,regular_skills,rare_skills,useless_skills]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ee35be00-99bd-4e49-9f06-6b3a335b18c9",
   "metadata": {
    "id": "ee35be00-99bd-4e49-9f06-6b3a335b18c9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Самая важная функция\n",
    "\n",
    "def skill_values(df,experience_id, is_vahta,industry_group,itog,sposob,q_base,q_regular,q_rare,ratio,lin_coef,method,year_coef):\n",
    "    #skill groups: base = 0,..., rare = 2\n",
    "\n",
    "    rr = ['new_salary','is_vahta','experience_id','industry_group','region_name']\n",
    "    skills_columns = [x for x in list(df) if x != 'id' and x not in rr]\n",
    "\n",
    "    #Получаем все навыки из функции выше\n",
    "    base_skills,regular_skills,rare_skills,useless_skills = skills_smart_decomposition(df,experience_id,is_vahta,industry_group,q_base,q_regular,q_rare,skills_columns,year_coef)\n",
    "\n",
    "    try:\n",
    "        if base_skills == 0:\n",
    "            return -1\n",
    "    except:\n",
    "        ...\n",
    "\n",
    "    #Весь код, который находится между двумя строками решеток нужен для того, чтобы корректно\n",
    "    # заполнялся список навыков у разных опытов одной профессии. Иначе говоря, все базовые навыки\n",
    "    # опыта i < j переходят к опыту j. Также базовые навыки предка ( на всякий случай ) удаляются\n",
    "    # из всех списков других навыков. Код между двумя линиями решеток делает то, что описано выше.\n",
    "\n",
    "#####################################################################################################\n",
    "    if experience_id == [0]: flag = 0\n",
    "    elif experience_id == [1]: flag = 1\n",
    "    else: flag = 2\n",
    "\n",
    "    base_skills_list = list(base_skills['skill'])\n",
    "    regular_skills_list = list(regular_skills['skill'])\n",
    "    rare_skills_list = list(rare_skills['skill'])\n",
    "\n",
    "    if industry_group != [0,1,2]:\n",
    "\n",
    "        if flag == 1:\n",
    "            try:\n",
    "                base_skills_list = list(set(base_skills_list + itog[is_vahta][industry_group[0]][0]['base_skills']))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "            except:\n",
    "                base_skills_list = list(set(base_skills_list))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "\n",
    "        if flag == 2:\n",
    "            try:\n",
    "                base_skills_list = list(set(base_skills_list + itog[is_vahta][industry_group[0]][1]['base_skills']))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "            except:\n",
    "                base_skills_list = list(set(base_skills_list))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "\n",
    "    else:\n",
    "\n",
    "        if flag == 1:\n",
    "            try:\n",
    "                base_skills_list = list(set(base_skills_list + itog[is_vahta][0]['base_skills']))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "            except:\n",
    "                base_skills_list = list(set(base_skills_list))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "\n",
    "        if flag == 2:\n",
    "            try:\n",
    "                base_skills_list = list(set(base_skills_list + itog[is_vahta][1]['base_skills']))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "            except:\n",
    "                base_skills_list = list(set(base_skills_list))\n",
    "                regular_skills_list = [x for x in regular_skills_list if x not in base_skills_list]\n",
    "                rare_skills_list = [x for x in rare_skills_list if x not in base_skills_list and x not in regular_skills_list]\n",
    "\n",
    "\n",
    "######################################################################################################\n",
    "\n",
    "\n",
    "    # Создаем датафрейм, который имеет хотя бы половину базовых навыков, а все остальные\n",
    "    # навыки равны нулю\n",
    "\n",
    "    df_base = df[(np.sum(df[base_skills_list],axis = 1) >= len(base_skills_list)//2)& \\\n",
    "                 (df[regular_skills_list].eq(0).all(axis=1))& \\\n",
    "                 (df[rare_skills_list].eq(0).all(axis=1))& \\\n",
    "                 (df[useless_skills['skill']].eq(0).all(axis=1))]\n",
    "\n",
    "    # Считаем базовую ЗП\n",
    "\n",
    "    if df_base.shape[0] == 0: return -1\n",
    "\n",
    "    if df_base.shape[0] == 1:\n",
    "        base_salary = round(np.median(df_base['new_salary']))\n",
    "    else:\n",
    "        base_salary = round(np.median(df_base[df_base['new_salary'] < \\\n",
    "                                              np.quantile(df_base['new_salary'],0.9)]['new_salary']))\n",
    "\n",
    "    # Фильтруем датафрейм под текущую выборку\n",
    "\n",
    "    dt = (df['is_vahta'] == is_vahta)&(df['experience_id'].isin(experience_id))&(df['industry_group'].isin(industry_group))\n",
    "\n",
    "    # Считаем макс ЗП в текущей выборке, удаляя очевидные выбросы\n",
    "\n",
    "    max_salary = round(np.max(df[dt][df[dt]['new_salary'] < np.quantile(df[dt]['new_salary'],0.98)]['new_salary']))\n",
    "\n",
    "    delta_salary = max_salary - base_salary\n",
    "\n",
    "\n",
    "    # Если редких скиллов нет, то делаем пустой датафрейм, чтобы было.\n",
    "    if rare_skills.shape[0] == 0:\n",
    "        ratio = 1\n",
    "        rare_skills = pd.DataFrame({'':0}, index = [1])\n",
    "\n",
    "\n",
    "    # 0 Метод подсчета стоимости навыка:\n",
    "    # Делим отведенную под группу навыков долю денег и делим на кол-во навыков\n",
    "    if method == 0:\n",
    "        regular_skill_values =  [round(delta_salary*ratio/ regular_skills.shape[0],2) \\\n",
    "                                                         for _ in range(regular_skills.shape[0])]\n",
    "        rare_skill_values = [round(delta_salary*\\\n",
    "                            (1-ratio) / rare_skills.shape[0],2) \\\n",
    "                            for _ in range(rare_skills.shape[0])]\n",
    "\n",
    "    # Cледующие методы в паре слов: чем популярнее, тем дороже\n",
    "    # или наоборот.\n",
    "\n",
    "\n",
    "    elif method == 1:\n",
    "        regular_skill_values =  linearize(np.array([round(delta_salary*ratio/ regular_skills.shape[0],2) \\\n",
    "                                                     for _ in range(regular_skills.shape[0])]),lin_coef)[::-1]\n",
    "        rare_skill_values = linearize(np.array([round(delta_salary*\\\n",
    "                        (1-ratio) / rare_skills.shape[0],2) \\\n",
    "                        for _ in range(rare_skills.shape[0])]),lin_coef)\n",
    "\n",
    "    else:\n",
    "        regular_skill_values =  linearize(np.array([round(delta_salary*ratio/ regular_skills.shape[0],2) \\\n",
    "                                                     for _ in range(regular_skills.shape[0])]),lin_coef)\n",
    "        rare_skill_values = linearize(np.array([round(delta_salary*\\\n",
    "                        (1-ratio) / rare_skills.shape[0],2) \\\n",
    "                        for _ in range(rare_skills.shape[0])]),lin_coef)[::-1]\n",
    "\n",
    "\n",
    "\n",
    "    # Сохранение результатов\n",
    "\n",
    "    reg_skill_dict = {}\n",
    "\n",
    "    for i,j in zip(regular_skill_values,regular_skills_list):\n",
    "        reg_skill_dict[j] = i\n",
    "\n",
    "    rare_skill_dict = {}\n",
    "\n",
    "    for i,j in zip(rare_skill_values,rare_skills_list):\n",
    "        rare_skill_dict[j] = i\n",
    "\n",
    "    reg_coefs = get_region_coefs(df[(df['experience_id'].isin(experience_id))&(df['is_vahta'] == is_vahta)],\\\n",
    "                                 is_vahta,experience_id,industry_group,sposob)\n",
    "\n",
    "\n",
    "    return { 'base_salary': base_salary, 'max_salary': max_salary,'region_coefs': reg_coefs,'regular_values':reg_skill_dict, 'rare_values': rare_skill_dict, 'base_skills':base_skills_list,'regular_skills':regular_skills_list, 'rare_skills':rare_skills_list,'useless_skills':list(useless_skills['skill'])}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "79813d5d-ac20-420f-b6c1-f01784542fbd",
   "metadata": {
    "id": "79813d5d-ac20-420f-b6c1-f01784542fbd",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Тут просто проходимся по каждой выборке вахта-опыт-регион и\n",
    "# вызываем предыдущую функцию.\n",
    "\n",
    "def train(df,sposob,q_base,q_regular,q_rare,ratio,lin_coef,method,year_coef):\n",
    "    itog = {0: {0:{0:dict(),1:dict(),2:dict()},\\\n",
    "                1:{0:dict(),1:dict(),2:dict()},\\\n",
    "                2:{0:dict(),1:dict(),2:dict()}},\\\n",
    "            1:{0:dict(),1:dict(),2:dict()}}\n",
    "\n",
    "\n",
    "    global gl_is_vahta\n",
    "    global gl_experience_id\n",
    "    global gl_industry_group\n",
    "\n",
    "\n",
    "    for is_vahta in [0,1]:\n",
    "        for experience_id in [[0],[1],[2,3]]:\n",
    "            if is_vahta == 0:\n",
    "                for industry_group in [[0],[1],[2]]:\n",
    "\n",
    "                    gl_is_vahta = is_vahta; gl_experience_id = experience_id; gl_industry_group = industry_group\n",
    "                    try:\n",
    "                        d = skill_values(df,experience_id,is_vahta,industry_group,itog,sposob,q_base,\\\n",
    "                                         q_regular,q_rare,ratio,lin_coef,method,year_coef)\n",
    "                    except:\n",
    "                        ...\n",
    "                    try:\n",
    "                        if d == -1:\n",
    "                            continue\n",
    "                    except:\n",
    "                        ...\n",
    "\n",
    "                    if experience_id ==[0]:\n",
    "                        itog[0][industry_group[0]][0] = d\n",
    "                    elif experience_id == [1]:\n",
    "                        itog[0][industry_group[0]][1] = d\n",
    "                    else:\n",
    "                        itog[0][industry_group[0]][2] = d\n",
    "\n",
    "            else:\n",
    "                gl_is_vahta = is_vahta; gl_experience_id = experience_id; gl_industry_group = -1\n",
    "                try:\n",
    "                    d = skill_values(df,experience_id,is_vahta,[0,1,2],itog,sposob,\\\n",
    "                                      q_base,q_regular,q_rare,ratio,lin_coef,method,year_coef)\n",
    "                except:\n",
    "                    ...\n",
    "                try:\n",
    "\n",
    "                    if d == -1:\n",
    "                        continue\n",
    "                except:\n",
    "                    ...\n",
    "                if experience_id ==[0]:\n",
    "                    itog[1][0] = d\n",
    "                elif experience_id == [1]:\n",
    "                    itog[1][1] = d\n",
    "                else:\n",
    "                    itog[1][2] = d\n",
    "\n",
    "    return itog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "34da500b-cdb1-4e3c-bc06-c8acdb0b7974",
   "metadata": {
    "id": "34da500b-cdb1-4e3c-bc06-c8acdb0b7974",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Для инференса нашей модели.\n",
    "# Считывает список навыков и по ключам вахта,опыт,регион\n",
    "# проходится по словарю с их стоимостями и складывает\n",
    "# домножает на кэф региона\n",
    "def model(mask,data):\n",
    "    #mask - лист с 0 и 1 где надо + регион\n",
    "    #regions - дикт с коэф-ми регионов\n",
    "    #data - основной дикт модели\n",
    "    vahta = mask[0]\n",
    "    exp = mask[1]\n",
    "    region = mask[2]\n",
    "\n",
    "    if str(int(vahta)) == '0':\n",
    "        industry_group = mask[3]\n",
    "        predict = data[vahta][industry_group][exp]['base_salary']\n",
    "        for i in mask[4:]:\n",
    "            # print(i)\n",
    "            # print(i in data[vahta][industry_group][exp]['regular_skills'],i in data[vahta][industry_group][exp]['rare_skills'],i in data[vahta][industry_group][exp]['useless_skills'])\n",
    "            if i in data[vahta][industry_group][exp]['regular_skills']:\n",
    "                predict += data[vahta][industry_group][exp]['regular_values'][i]\n",
    "                # print(predict)\n",
    "                continue\n",
    "            if i in data[vahta][industry_group][exp]['rare_skills']:\n",
    "                predict += data[vahta][industry_group][exp]['rare_values'][i]\n",
    "                # print(predict)\n",
    "                continue\n",
    "        try:\n",
    "            reg_cef = data[vahta][industry_group][exp]['region_coefs'][region]\n",
    "        except:\n",
    "            reg_cef = 1\n",
    "        return predict * reg_cef\n",
    "\n",
    "    else:\n",
    "        predict = data[vahta][exp]['base_salary']\n",
    "        for i in mask[4:]:\n",
    "            if i in data[vahta][exp]['regular_skills']:\n",
    "                predict += data[vahta][exp]['regular_values'][i]\n",
    "                continue\n",
    "            if i in data[vahta][exp]['rare_skills']:\n",
    "                predict += data[vahta][exp]['rare_values'][i]\n",
    "                continue\n",
    "        try:\n",
    "            reg_cef = data[vahta][exp]['region_coefs'][region]\n",
    "        except:\n",
    "            reg_cef = 1\n",
    "        return predict * reg_cef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "20e3725d-1b59-477f-a300-7bd5f37c6f71",
   "metadata": {
    "id": "20e3725d-1b59-477f-a300-7bd5f37c6f71",
    "tags": []
   },
   "outputs": [],
   "source": [
    "rmse =lambda y_test,y_pred: mean_squared_error(y_test, y_pred)** 0.5\n",
    "mape = lambda y_test,y_pred: mean_absolute_percentage_error(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "07a0ac28-1cd0-4f36-89a3-a6b26eb14043",
   "metadata": {
    "id": "07a0ac28-1cd0-4f36-89a3-a6b26eb14043",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# подсчет специальной метрики\n",
    "\n",
    "def new_new_metric(true_values, predicted_value,metric_X,metric_Y, df_):\n",
    "    true_values = np.array(true_values)\n",
    "    n = len(true_values)\n",
    "\n",
    "    deviation = abs(true_values - predicted_value) / true_values * 100\n",
    "\n",
    "    for percent in [1, 3, 5, 10, 15, 20, 25]:\n",
    "        df_[f'не более {percent}%'] = deviation <= percent\n",
    "        count_val = len(deviation[deviation <= percent])\n",
    "        metric_X[f'не более {percent}%'] += count_val\n",
    "    for percent in [200, 150, 100, 75, 50, 25]:\n",
    "        df_[f'более {percent}%'] = deviation > percent\n",
    "        count_val = len(deviation[deviation > percent])\n",
    "        metric_Y[f'более {percent}%'] += count_val\n",
    "    return metric_X, metric_Y, df_\n",
    "\n",
    "\n",
    "\n",
    "def validate_XY(true_values,predicted_values):\n",
    "    metric_X, metric_Y = {}, {}\n",
    "    res_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "    for percent in [1, 3, 5, 10, 15, 20, 25]:\n",
    "        count_val = 0\n",
    "        metric_X[f'не более {percent}%'] = count_val\n",
    "    for percent in [200, 150, 100, 75, 50, 25]:\n",
    "        count_val = 0\n",
    "        metric_Y[f'более {percent}%'] = count_val\n",
    "\n",
    "    metric_X, metric_Y, res_df = new_new_metric(true_values,predicted_values,metric_X,metric_Y,res_df)\n",
    "\n",
    "    return metric_X, metric_Y, res_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "18cdf018-8985-4612-8246-4752451deac7",
   "metadata": {
    "id": "18cdf018-8985-4612-8246-4752451deac7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cb_kfold(df,params = 0):\n",
    "    n_splits = 3  # Число фолдов. Оптимально 3, 5 или 10\n",
    "    regs = [] # Тут будем сохранять модели\n",
    "    scores = [] # Тут будем хранить скоры валидационных фолдов\n",
    "\n",
    "    # параметры валидации, обучение будет идти на n_splits фолдах\n",
    "    X = df.drop('new_salary', axis=1, errors=\"ignore\")\n",
    "    y = df[\"new_salary\"]\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=7575)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Специальный класс для ускорения обучения\n",
    "        train_dataset = Pool(data=X_train, label=y_train, cat_features=['region_name'])\n",
    "        eval_dataset = Pool(data=X_test, label=y_test, cat_features=['region_name'])\n",
    "\n",
    "        if params:\n",
    "            reg = CatBoostRegressor(cat_features = ['region_name'],\n",
    "                                    iterations=6000,\n",
    "                                    **params)\n",
    "        else:\n",
    "            reg = CatBoostRegressor(cat_features=['region_name'],\n",
    "                                     eval_metric = 'MAPE',\n",
    "                                    depth=6,\n",
    "                                    iterations=6000,\n",
    "                                   # Регуляризация и ускорение\n",
    "                                    colsample_bylevel=0.098,\n",
    "                                    subsample=0.95,\n",
    "                                    l2_leaf_reg=9,\n",
    "                                    min_data_in_leaf=243,\n",
    "                                    max_bin=187,\n",
    "                                    random_strength=1,\n",
    "\n",
    "                                    # Параметры скорения\n",
    "                                    task_type=\"CPU\",\n",
    "                                    thread_count=-1,\n",
    "                                    bootstrap_type=\"Bernoulli\",\n",
    "                                    per_float_feature_quantization=['2:border_count=1024'],\n",
    "\n",
    "\n",
    "                                    # Важное!\n",
    "                                    random_seed=7575)\n",
    "        regs.append(reg)\n",
    "\n",
    "        reg.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=(X_test, y_test),\n",
    "            verbose=False,\n",
    "            use_best_model=True,\n",
    "            plot=False,\n",
    "            early_stopping_rounds=200,\n",
    "        )\n",
    "\n",
    "    scores.append(np.mean([v for k, v in reg.best_score_[\"validation\"].items() if \"MAPE\" in k], dtype=\"float16\"))\n",
    "\n",
    "\n",
    "    mean_mape = np.mean(scores, dtype=\"float16\") - np.std(scores, dtype=\"float16\")\n",
    "\n",
    "    return regs,mean_mape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "411aabf1-271a-4740-bbd5-f37b8b70296f",
   "metadata": {
    "id": "411aabf1-271a-4740-bbd5-f37b8b70296f",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def cb_rs(df, M ):\n",
    "\n",
    "\n",
    "    train_pool = Pool(df.drop('new_salary',axis =1 ), df.new_salary, cat_features=['region_name'])\n",
    "\n",
    "    model = CatBoostRegressor(\n",
    "    iterations=4000,\n",
    "    loss_function = 'RMSE',\n",
    "    early_stopping_rounds = 150,\n",
    "    # Параметры скорения\n",
    "    task_type=\"CPU\",\n",
    "    thread_count=-1,\n",
    "\n",
    "    # Важное!\n",
    "    random_seed=7575,\n",
    "    verbose = 3999)\n",
    "\n",
    "    param_distribution = {\n",
    "    'depth': [5, 6],\n",
    "    'l2_leaf_reg': [1, 3, 5],\n",
    "    'bagging_temperature': [0.5, 1.0, 1.5],\n",
    "    'random_strength': [0.1, 0.5, 1.0],\n",
    "    'one_hot_max_size': [2, 5, 10],\n",
    "    'colsample_bylevel': [0.5, 0.8, 0.01],\n",
    "    'leaf_estimation_iterations': [5, 10, 15],\n",
    "    'max_ctr_complexity': [1, 3, 5],\n",
    "    'grow_policy': ['SymmetricTree', 'Depthwise'],\n",
    "    'min_data_in_leaf': [1, 3, 5],\n",
    "    'leaf_estimation_method': ['Newton', 'Gradient'],\n",
    "    # 'max_leaves': [30, 50, 100],\n",
    "    'max_bin': [128, 256, 512]\n",
    "    }\n",
    "\n",
    "    randomized_search_results = model.randomized_search(\n",
    "        param_distribution,\n",
    "        train_pool,\n",
    "        cv=3,\n",
    "        n_iter=M,\n",
    "        shuffle=False,\n",
    "        plot=False)\n",
    "\n",
    "\n",
    "\n",
    "    return randomized_search_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "id": "72ada94b-508c-4e5d-bdfa-4bea632281f2",
   "metadata": {
    "id": "72ada94b-508c-4e5d-bdfa-4bea632281f2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_cb(dt_,cb_regs):\n",
    "    pred_0 = cb_regs[0].predict(dt_)\n",
    "    pred_1 = cb_regs[1].predict(dt_)\n",
    "    pred_2 = cb_regs[2].predict(dt_)\n",
    "\n",
    "    return (pred_0 + pred_1 + pred_2) /3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "66b758aa-929c-47fe-afd5-0d05120367a2",
   "metadata": {
    "id": "66b758aa-929c-47fe-afd5-0d05120367a2",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def validate_cb(dt_,cb_regs):\n",
    "\n",
    "    rr = ['new_salary','is_vahta','experience_id','industry_group','region_name','year']\n",
    "    skills_columns = [x for x in list(dt_) if x != 'id' and x not in rr]\n",
    "    actuals_full = list(dt_['new_salary'])\n",
    "    dt_ = dt_.drop('new_salary',axis = 1)\n",
    "\n",
    "    pred_0 = cb_regs[0].predict(dt_)\n",
    "    pred_1 = cb_regs[1].predict(dt_)\n",
    "    pred_2 = cb_regs[2].predict(dt_)\n",
    "\n",
    "\n",
    "\n",
    "    y_pred = (pred_0 + pred_1 + pred_2) /3\n",
    "\n",
    "    return actuals_full,y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "a94e5977-20f4-4a02-bcb1-d1543c7441b6",
   "metadata": {
    "id": "a94e5977-20f4-4a02-bcb1-d1543c7441b6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_model(dt_1,mod,actuals = np.array([])):\n",
    "\n",
    "    cb_preds1 = []\n",
    "    model_preds1 = []\n",
    "    actual = []\n",
    "    mask1 = list(dt_1)\n",
    "    for ind in range(dt_1.shape[0]):\n",
    "        is_vahta = dt_1.is_vahta.iloc[ind]\n",
    "        exp_id = dt_1.experience_id.iloc[ind]\n",
    "        regn = dt_1.region_name.iloc[ind]\n",
    "        indg = dt_1.industry_group.iloc[ind]\n",
    "        try:\n",
    "        # print([is_vahta,exp_id,regn,indg])\n",
    "            model_preds1 += [model([is_vahta,exp_id,regn,indg] +\\\n",
    "                               list(dt_1[np.array(list(dt_1[mask1[1:]]))[list(dt_1[mask1[1:]].eq(1).iloc[ind])]]) ,mod)]\n",
    "        except:\n",
    "            print('validate e model error')\n",
    "            continue\n",
    "        actual += [actuals[ind]]\n",
    "\n",
    "    return model_preds1,actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "cda43b4a-7604-49e7-88f2-6b5794815339",
   "metadata": {
    "id": "cda43b4a-7604-49e7-88f2-6b5794815339",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bench_test(cb_p,mod_p,coef):\n",
    "    ratio = 1\n",
    "    for i in range(1,cb_p.shape[0]):\n",
    "        if (cb_p[i]*(1 - coef) + mod_p[i] * coef) >= (cb_p[i-1]*(1 - coef) + mod_p[i-1] * coef):\n",
    "            ratio +=1\n",
    "    return(ratio/cb_p.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "770ea80c-d972-4d4c-8358-ac0bf84bbd49",
   "metadata": {
    "id": "770ea80c-d972-4d4c-8358-ac0bf84bbd49",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mod - model\n",
    "# cb_m - catboost models\n",
    "# dt_ - test sample\n",
    "def get_predicts_both(dt_1,mod,cb_m,actuals = np.array([])):\n",
    "\n",
    "    cb_preds1 = []\n",
    "    model_preds1 = []\n",
    "    actual = []\n",
    "    mask1 = list(dt_1)\n",
    "    for ind in range(dt_1.shape[0]):\n",
    "        is_vahta = dt_1.is_vahta.iloc[ind]\n",
    "        exp_id = dt_1.experience_id.iloc[ind]\n",
    "        regn = dt_1.region_name.iloc[ind]\n",
    "        indg = dt_1.industry_group.iloc[ind]\n",
    "        try:\n",
    "        # print([is_vahta,exp_id,regn,indg])\n",
    "            model_preds1 += [model([is_vahta,exp_id,regn,indg] +\\\n",
    "                               list(dt_1[np.array(list(dt_1[mask1[1:]]))[list(dt_1[mask1[1:]].eq(1).iloc[ind])]]) ,mod)]\n",
    "        except:\n",
    "            cb_preds1 += [None]\n",
    "            actual += [None]\n",
    "            model_preds1 += [None]\n",
    "            continue\n",
    "\n",
    "        if actuals.shape[0] > 1:\n",
    "            actual += [actuals[ind]]\n",
    "        cb_preds1 += [pred_cb(dt_1.iloc[[ind]], cb_m)]\n",
    "\n",
    "    if actuals.shape[0] > 1:\n",
    "        return cb_preds1,model_preds1,actual\n",
    "    else:\n",
    "        return cb_preds1,model_preds1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "f282ac6d-1b2e-4200-89bf-881efc69c72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# mod - model\n",
    "# cb_m - catboost models\n",
    "# dt_ - test sample\n",
    "def get_predicts_model(dt_1,mod):\n",
    "\n",
    "    model_preds1 = []\n",
    "    mask1 = list(dt_1)\n",
    "    for ind in range(dt_1.shape[0]):\n",
    "        is_vahta = dt_1.is_vahta.iloc[ind]\n",
    "        exp_id = dt_1.experience_id.iloc[ind]\n",
    "        regn = dt_1.region_name.iloc[ind]\n",
    "        indg = dt_1.industry_group.iloc[ind]\n",
    "        try:\n",
    "        # print([is_vahta,exp_id,regn,indg])\n",
    "            model_preds1 += [model([is_vahta,exp_id,regn,indg] +\\\n",
    "                               list(dt_1[np.array(list(dt_1[mask1[1:]]))[list(dt_1[mask1[1:]].eq(1).iloc[ind])]]) ,mod)]\n",
    "        except:\n",
    "            model_preds1 += [pd.NA]\n",
    "\n",
    "    return model_preds1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "f55f32c8-0a88-44b0-922a-4f7b4002b3e9",
   "metadata": {
    "id": "f55f32c8-0a88-44b0-922a-4f7b4002b3e9",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_bench(skills_columns):\n",
    "    size = len(skills_columns)\n",
    "    array = np.zeros((size, size), dtype=object)\n",
    "    for i in range(size):\n",
    "        array[i, :i+1] = 1\n",
    "\n",
    "    bench_l = []\n",
    "    for ye in [2018,2019,2022,2023]:\n",
    "        for expid in [0,1,2]:\n",
    "            for indgru in [0,1,2]:\n",
    "                bench_l += [[ye,0,expid,'Город федерального значения Москва',indgru] for _ in range(size)]\n",
    "    bench_l = np.array(bench_l,dtype = object)\n",
    "\n",
    "    array = np.tile(array, (bench_l.shape[0]//size,1))\n",
    "    result = np.concatenate((bench_l, array), axis=1)\n",
    "    return pd.DataFrame(result,columns = ['year','is_vahta','experience_id','region_name','industry_group']+skills_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "a94463b4-1a7b-4710-9011-18e4b1030c2c",
   "metadata": {
    "id": "a94463b4-1a7b-4710-9011-18e4b1030c2c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_us_skills(trained_model):\n",
    "    useless_skills = []\n",
    "    for vht in [0,1]:\n",
    "        for indg in [0,1,2]:\n",
    "\n",
    "            try:\n",
    "                useless_skills += trained_model[vht][indg][2]['useless_skills']\n",
    "            except:\n",
    "                ...\n",
    "\n",
    "    return list(np.unique(useless_skills))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "d77ee423-e660-41f0-b94d-55656b43a08a",
   "metadata": {
    "id": "d77ee423-e660-41f0-b94d-55656b43a08a",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# скрипт для заполнения пустых диктов в модели\n",
    "def fill_model(trmodel):\n",
    "    for ind in [0,1,2]:\n",
    "        for exp in [0,1,2]:\n",
    "            if len(trmodel[0][ind][exp]) == 0:\n",
    "                for exp_i in [exp] + [_ for _ in [2,1,0] if _ != exp]:\n",
    "                    for ind_i in [ind] + [_ for _ in [2,1,0] if _ != ind]:\n",
    "                        if len(trmodel[0][ind_i][exp_i]) != 0:\n",
    "                            trmodel[0][ind][exp] = trmodel[0][ind_i][exp_i]\n",
    "\n",
    "\n",
    "    return trmodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "55a92c81-9735-4a64-a149-ea7e2e0cb866",
   "metadata": {
    "id": "55a92c81-9735-4a64-a149-ea7e2e0cb866",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Функция, которая конвертирует ключи 3 layer словаря в инты\n",
    "def convert_keys_to_int(d):\n",
    "    if isinstance(d, dict):\n",
    "        new_dict = {}\n",
    "        for key, value in d.items():\n",
    "            if isinstance(key, str):\n",
    "                try:\n",
    "                    new_key = int(key)\n",
    "                except ValueError:\n",
    "                    new_key = key\n",
    "            else:\n",
    "                new_key = key\n",
    "\n",
    "            new_value = convert_keys_to_int(value)\n",
    "            new_dict[new_key] = new_value\n",
    "\n",
    "        return new_dict\n",
    "    elif isinstance(d, list):\n",
    "        return [convert_keys_to_int(item) for item in d]\n",
    "    else:\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "b0eed63c-4883-4a0d-8810-316f19d65593",
   "metadata": {
    "id": "b0eed63c-4883-4a0d-8810-316f19d65593",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_metric_rate(target_column,comparison_column, df):\n",
    "\n",
    "    salary_column = comparison_column\n",
    "    \n",
    "    columns_to_drop = list(df)\n",
    "    columns_to_drop.remove(target_column)\n",
    "\n",
    "\n",
    "    df_j = df.join(validate_XY(df[salary_column], df.salary_predicted)[2])\n",
    "    df_j =  df_j[df_j['salary_predicted'].notna()]\n",
    "    df_j['N'] = 1\n",
    "\n",
    "    new_metric_col = ['не более 1%', 'не более 3%', 'не более 5%', 'не более 10%', 'не более 15%', 'не более 20%',\n",
    "                      'не более 25%', 'более 200%', 'более 150%', 'более 100%',\n",
    "                      'более 75%', 'более 50%', 'более 25%']\n",
    "\n",
    "    new_df = df_j.drop(columns=columns_to_drop).groupby(target_column).sum()\n",
    "\n",
    "    for value in list(new_df.index):\n",
    "        y_test = df_j[df_j[target_column] == value][salary_column]; y_pred = df_j[df_j[target_column] == value].salary_predicted\n",
    "        try:\n",
    "            table_rmse = rmse(y_test, y_pred)\n",
    "        except:\n",
    "            table_rmse = pd.NA\n",
    "        new_df.loc[value, 'rmse'] = table_rmse\n",
    "\n",
    "        try:\n",
    "            table_mape = mape(y_test, y_pred)\n",
    "        except:\n",
    "            table_mape = pd.NA\n",
    "\n",
    "        new_df.loc[value, 'mape'] = table_mape\n",
    "\n",
    "        new_df.loc[value, new_metric_col] /= new_df.loc[value, ['не более 25%', 'более 25%']].sum()\n",
    "        new_df.loc[value, new_metric_col] = new_df.loc[value, new_metric_col].apply(lambda x: round(x,3))\n",
    "\n",
    "\n",
    "\n",
    "        new_df = new_df[['N','rmse','mape'] + [x for x in list(new_df) if x not in ['N','rmse','mape']]]\n",
    "\n",
    "\n",
    "\n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "A19ayAl2nqvM",
   "metadata": {
    "id": "A19ayAl2nqvM",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# код, который строит распределение уникальных значений в столбце (тип по OX - уникальные значения, по OY - их частота)\n",
    "\n",
    "def make_chart(df, column_name, save_path):\n",
    "    # Получение уникальных значений и их количества\n",
    "    unique_values = df[column_name].value_counts().reset_index()\n",
    "    unique_values.columns = [column_name, 'count']\n",
    "\n",
    "    # Построение графика\n",
    "    fig = px.bar(unique_values, x=column_name, y='count', title=f'Distribution of {column_name}')\n",
    "\n",
    "    # Сохранение графика в виде изображения\n",
    "    #fig.show()\n",
    "    fig.write_image(save_path, engine='kaleido')\n",
    "\n",
    "# column_name = 'region_name'\n",
    "# save_path = f'/content/drive/MyDrive/solovey/20.08.2023/results3/{column_name}.png'\n",
    "# df = pd.read_csv(f'/content/drive/MyDrive/solovey/20.08.2023/results3/final_dfs/{833}.csv')\n",
    "# make_chart(df, column_name, save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "rCUhLJUlrkkL",
   "metadata": {
    "id": "rCUhLJUlrkkL",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_salary_distribution(df, save_path,labels):\n",
    "    #labels = [title, xlabel, ylabel]\n",
    "    fig, ax = plt.subplots()  # Create a figure and axis\n",
    "\n",
    "    # Plot the actual salary distribution\n",
    "    sns.histplot(data=df, x='salary_actual', ax=ax, label='Actual', alpha=0.7)\n",
    "\n",
    "    # Plot the predicted salary distribution\n",
    "    sns.histplot(data=df, x='salary_predicted', ax=ax, label='Predicted', alpha=0.7)\n",
    "\n",
    "    ax.set_title(labels[0])\n",
    "    ax.set_xlabel(labels[1])\n",
    "    ax.set_ylabel(labels[2])\n",
    "    ax.legend()\n",
    "\n",
    "    # Save the plot as an image\n",
    "    fig.savefig(save_path, format='png', dpi=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "-wTA0cq50wSC",
   "metadata": {
    "id": "-wTA0cq50wSC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def plot_only_salary(df, column, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    # Построение гистограммы с KDE\n",
    "    sns.histplot(data=df, x=column, bins=20, kde=True, label=f'{column}')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Salary')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title(f'{column} distribution')\n",
    "\n",
    "    # Сохранение графика\n",
    "    plt.savefig(save_path)\n",
    "    #plt.show()\n",
    "\n",
    "# save_path = '/content/drive/MyDrive/solovey/temp.png'\n",
    "# plot_only_salary(df, 'salary_actual', save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "bJxdgQ0MCxvf",
   "metadata": {
    "id": "bJxdgQ0MCxvf",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# правка номер 1031031293193810923\n",
    "def plot_only_salary2(df, column, save_path):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    if column == 'salary_actual':\n",
    "        sns.histplot(data=df, x=column, bins=20, kde=True, label=f'{column}')\n",
    "\n",
    "\n",
    "    elif column == 'salary_predicted':\n",
    "        sns.histplot(data=df, x='salary_actual', bins=20, kde=True, label='salary_actual', palette='bright')\n",
    "        # Построение гистограммы с KDE для второго столбца\n",
    "        sns.histplot(data=df, x='salary_predicted', bins=20, kde=True, label='salary_predicted', color='orange', alpha=0.4, edgecolor='orange')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.xlabel('Salary')\n",
    "    plt.ylabel('frequency')\n",
    "    plt.title(f'{column} distribution')\n",
    "    plt.savefig(save_path)\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# plot_only_salary2(df, 'salary_predicted', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "eaa4f8b7-329e-4e49-aeba-d167b7db9f36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_7(df):\n",
    "    df = df.fillna(df.mean())\n",
    "    salary_column = 'salary_actual' # по какому столбцу сравниваем\n",
    "\n",
    "    df['rmse'] = df.apply(lambda row: mean_squared_error([row[salary_column]], [row['salary_predicted']], squared=False), axis=1)\n",
    "    df['mape'] = df.apply(lambda row: mean_absolute_percentage_error([row[salary_column]], [row['salary_predicted']]), axis=1)\n",
    "    metric_X, metric_Y, persent_df = validate_XY(df[salary_column], df['salary_predicted'])\n",
    "    df = pd.concat([df, persent_df], axis=1)\n",
    "\n",
    "    # ax = sns.histplot(data=df, x=salary_column, bins=10, kde=True, label=salary_column, palette='bright')\n",
    "    # hist_data = ax.patches\n",
    "    # bin_edges = [patch.get_x() for patch in hist_data]\n",
    "    # bin_edges.append(hist_data[-1].get_x() + hist_data[-1].get_width())\n",
    "\n",
    "    borders_salary = np.linspace(min(df[df[salary_column]>5000][salary_column]),max(df[df[salary_column]<250000][salary_column]), 15)\n",
    "\n",
    "    borders_salary_list = [f'{round(float(borders_salary[i-1]),2)} - {round(float(borders_salary[i]),2)}' for i in range(1,10)]\n",
    "    N_list = []\n",
    "    mape_list = []\n",
    "    rmse_list = []\n",
    "    persents_lists = [[] for _ in range(len(persent_df.columns.tolist()))]\n",
    "\n",
    "    for index, border in enumerate(borders_salary_list):\n",
    "        if index == len(borders_salary)-1:\n",
    "            temp_df = df[(df[salary_column]>=float(border.split(' - ')[0]))&(df[salary_column]<=float(border.split(' - ')[1]))]\n",
    "        else:\n",
    "            temp_df = df[(df[salary_column]>=float(border.split(' - ')[0]))&(df[salary_column]<float(border.split(' - ')[1]))]\n",
    "\n",
    "        N_list.append(temp_df.shape[0])\n",
    "        mape_list.append(np.round(np.mean(temp_df['mape'].values),2))\n",
    "        rmse_list.append(np.round(np.mean(temp_df['rmse'].values)))\n",
    "\n",
    "        for index2, name_pers in enumerate(persent_df.columns.tolist()):\n",
    "            val = temp_df[name_pers].sum()/temp_df[name_pers].shape[0]\n",
    "            persents_lists[index2].append(np.round(val,2))\n",
    "\n",
    "\n",
    "\n",
    "    final_df = pd.DataFrame(columns = ['Группа ЗП', 'N', 'mape', 'rmse'] + persent_df.columns.tolist())\n",
    "    final_df['Группа ЗП'] = borders_salary_list\n",
    "    final_df['N'] = N_list\n",
    "    final_df['mape'] = mape_list\n",
    "    final_df['rmse'] = rmse_list\n",
    "\n",
    "    for index, name in enumerate(persent_df.columns.tolist()):\n",
    "        final_df[name] = persents_lists[index]\n",
    "\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "id": "007e2af4-346a-4298-b4e2-6aba9333a1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bundles = [663, 767, 783, 833, 27, 31, 52, 336, 16, 33, 49, 283, 54, 58, 61, 69, 74, 116, 119, 128, 706, 836, 919, 91, 134]\n",
    "\n",
    "coef_dict = {n:0.1 for n in bundles}\n",
    "coef_dict[33] = 0.3\n",
    "coef_dict[61] = 0.4\n",
    "coef_dict[663] = 0.2\n",
    "coef_dict[706] = 0.3\n",
    "coef_dict[767] = 0.3\n",
    "coef_dict[91] = 0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61797b0",
   "metadata": {},
   "source": [
    "# Моя задача (пункт 2)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "a7237a59",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e03c37",
   "metadata": {},
   "source": [
    "план работы:   \n",
    "работем только с 16 бундлом\n",
    "1) сделать на новых данных предсказание, сохранить в папку predicted\n",
    "2) посмотреть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "3ebfc998",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experience_id</th>\n",
       "      <th>N</th>\n",
       "      <th>rmse</th>\n",
       "      <th>mape</th>\n",
       "      <th>не более 1%</th>\n",
       "      <th>не более 3%</th>\n",
       "      <th>не более 5%</th>\n",
       "      <th>не более 10%</th>\n",
       "      <th>не более 15%</th>\n",
       "      <th>не более 20%</th>\n",
       "      <th>не более 25%</th>\n",
       "      <th>более 200%</th>\n",
       "      <th>более 150%</th>\n",
       "      <th>более 100%</th>\n",
       "      <th>более 75%</th>\n",
       "      <th>более 50%</th>\n",
       "      <th>более 25%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>10682</td>\n",
       "      <td>10879.516477</td>\n",
       "      <td>0.190254</td>\n",
       "      <td>0.035</td>\n",
       "      <td>0.112</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.359</td>\n",
       "      <td>0.503</td>\n",
       "      <td>0.620</td>\n",
       "      <td>0.730</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.011</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>25536</td>\n",
       "      <td>14147.184502</td>\n",
       "      <td>0.195652</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0.084</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.305</td>\n",
       "      <td>0.493</td>\n",
       "      <td>0.611</td>\n",
       "      <td>0.710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.009</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.0</td>\n",
       "      <td>13581</td>\n",
       "      <td>13965.984049</td>\n",
       "      <td>0.181516</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.162</td>\n",
       "      <td>0.307</td>\n",
       "      <td>0.546</td>\n",
       "      <td>0.645</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.796</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.012</td>\n",
       "      <td>0.061</td>\n",
       "      <td>0.092</td>\n",
       "      <td>0.204</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   experience_id      N          rmse      mape  не более 1%  не более 3%   \n",
       "0            0.0  10682  10879.516477  0.190254        0.035        0.112  \\\n",
       "1            1.0  25536  14147.184502  0.195652        0.025        0.084   \n",
       "2            2.0  13581  13965.984049  0.181516        0.040        0.162   \n",
       "\n",
       "   не более 5%  не более 10%  не более 15%  не более 20%  не более 25%   \n",
       "0        0.180         0.359         0.503         0.620         0.730  \\\n",
       "1        0.143         0.305         0.493         0.611         0.710   \n",
       "2        0.307         0.546         0.645         0.728         0.796   \n",
       "\n",
       "   более 200%  более 150%  более 100%  более 75%  более 50%  более 25%  \n",
       "0         0.0       0.002       0.005      0.011      0.047      0.270  \n",
       "1         0.0       0.001       0.003      0.009      0.044      0.290  \n",
       "2         0.0       0.001       0.012      0.061      0.092      0.204  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = './dfs_report/16.csv'\n",
    "target_column = 'industry_group'\n",
    "\n",
    "def get_metric_rate(target_column, df):\n",
    "\n",
    "    salary_column = 'salary_actual' # по чему сравниваем\n",
    "\n",
    "    columns_to_drop = list(df)\n",
    "    columns_to_drop.remove(target_column)\n",
    "\n",
    "\n",
    "    #df_j = df.join(validate_XY(df[salary_column], df.salary_predicted)[2])\n",
    "    df_j = df.copy()\n",
    "    df_j =  df_j[df_j['salary_predicted'].notna()]\n",
    "    df_j['N'] = 1\n",
    "\n",
    "    new_metric_col = ['не более 1%', 'не более 3%', 'не более 5%', 'не более 10%', 'не более 15%', 'не более 20%',\n",
    "                        'не более 25%', 'более 200%', 'более 150%', 'более 100%',\n",
    "                        'более 75%', 'более 50%', 'более 25%']\n",
    "\n",
    "    new_df = df_j.groupby(target_column).sum()\n",
    "\n",
    "    for value in list(new_df.index):\n",
    "        y_test = df_j[df_j[target_column] == value][salary_column]; y_pred = df_j[df_j[target_column] == value].salary_predicted\n",
    "        try:\n",
    "            table_rmse = rmse(y_test, y_pred)\n",
    "        except:\n",
    "            table_rmse = pd.NA\n",
    "        new_df.loc[value, 'rmse'] = table_rmse\n",
    "        \n",
    "        try:\n",
    "            table_mape = mape(y_test, y_pred)\n",
    "        except:\n",
    "            table_mape = pd.NA\n",
    "\n",
    "\n",
    "        new_df.loc[value, 'mape'] = table_mape\n",
    "        \n",
    "        new_df.loc[value, new_metric_col] /= new_df.loc[value, ['не более 25%', 'более 25%']].sum()\n",
    "        new_df.loc[value, new_metric_col] = new_df.loc[value, new_metric_col].apply(lambda x: round(x,3))\n",
    "\n",
    "    new_df = new_df[['N', 'rmse', 'mape']+new_metric_col]\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df \n",
    "\n",
    "\n",
    "path = './dfs_report/16.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df[df['salary_actual']>5000]\n",
    "target_column = 'experience_id'\n",
    "look_df = get_metric_rate(target_column, df)\n",
    "look_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "d5e21a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bundles = [116, 119, 124, 128, 134, 16, 23, 27, 283, 31, 33, 336, 357, 43,49,52,54, 561, 58, 61, 663, 69, 693, 706, 712, 74, 742, \\\n",
    " 765, 767, 783, 811, 812, 833, 836, 84, 85, 852, 853, 856, 87, 873, 876, 888, 898, 91, 917, 919, 921, 954, 992]\n",
    "\n",
    "for n_bundle in Bundles:\n",
    "    try:\n",
    "        path = f'./dfs_report/{n_bundle}.csv'\n",
    "        df = pd.read_csv(path)\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "    df = df[df['salary_actual']>5000]\n",
    "    dict_bad = {}\n",
    "    for target_column in ['experience_id']:\n",
    "        look_df = get_metric_rate(target_column, df)\n",
    "        look_df_small_mape = look_df[look_df['mape']>=0.3]\n",
    "        # в словарь сохраняем все значения target_column при которых mape>=0.3\n",
    "        dict_bad[target_column] = look_df_small_mape[target_column].values.tolist()\n",
    "\n",
    "    if dict_bad['experience_id'] != []:\n",
    "        print(n_bundle)\n",
    "        # break \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ed19d7",
   "metadata": {},
   "source": [
    "# МОДЕЛИ КАТБУСТА В РАЗРЕЗЕ ОПЫТА"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "13f1a880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 []\n",
      "119 []\n",
      "128 []\n",
      "134 []\n",
      "16 []\n",
      "27 []\n",
      "283 [0.0, 1.0, 2.0]\n",
      "31 []\n",
      "33 []\n",
      "336 []\n",
      "49 []\n",
      "52 []\n",
      "54 []\n",
      "58 []\n",
      "61 [0.0]\n",
      "663 [0.0]\n",
      "69 []\n",
      "706 []\n",
      "74 []\n",
      "767 []\n",
      "783 []\n",
      "833 []\n",
      "836 [0.0]\n",
      "91 []\n",
      "919 []\n"
     ]
    }
   ],
   "source": [
    "# ТУТ ВСЁ\n",
    "\n",
    "def get_metric_rate(target_column, df):\n",
    "\n",
    "    salary_column = 'salary_actual' # по чему сравниваем\n",
    "\n",
    "    columns_to_drop = list(df)\n",
    "    columns_to_drop.remove(target_column)\n",
    "\n",
    "\n",
    "    #df_j = df.join(validate_XY(df[salary_column], df.salary_predicted)[2])\n",
    "    df_j = df.copy()\n",
    "    df_j =  df_j[df_j['salary_predicted'].notna()]\n",
    "    df_j['N'] = 1\n",
    "\n",
    "    new_metric_col = ['не более 1%', 'не более 3%', 'не более 5%', 'не более 10%', 'не более 15%', 'не более 20%',\n",
    "                        'не более 25%', 'более 200%', 'более 150%', 'более 100%',\n",
    "                        'более 75%', 'более 50%', 'более 25%']\n",
    "\n",
    "    new_df = df_j.groupby(target_column).sum()\n",
    "\n",
    "    for value in list(new_df.index):\n",
    "        y_test = df_j[df_j[target_column] == value][salary_column]; y_pred = df_j[df_j[target_column] == value].salary_predicted\n",
    "        try:\n",
    "            table_rmse = rmse(y_test, y_pred)\n",
    "        except:\n",
    "            table_rmse = pd.NA\n",
    "        new_df.loc[value, 'rmse'] = table_rmse\n",
    "        \n",
    "        try:\n",
    "            table_mape = mape(y_test, y_pred)\n",
    "        except:\n",
    "            table_mape = pd.NA\n",
    "            \n",
    "        new_df.loc[value, 'mape'] = table_mape\n",
    "        \n",
    "        new_df.loc[value, new_metric_col] /= new_df.loc[value, ['не более 25%', 'более 25%']].sum()\n",
    "        new_df.loc[value, new_metric_col] = new_df.loc[value, new_metric_col].apply(lambda x: round(x,3))\n",
    "\n",
    "    new_df = new_df[['N', 'rmse', 'mape']+new_metric_col]\n",
    "    new_df.reset_index(inplace=True)\n",
    "    return new_df \n",
    "\n",
    "\n",
    "    \n",
    "def cb_for_bad(df):\n",
    "\n",
    "    n_splits = 3  # Число фолдов\n",
    "    regs = [] # тут сохр модели\n",
    "    scores = [] # тут будем хранить скоры валидационных фолдов\n",
    "\n",
    "    # параметры валидации, обучение будет идти на n_splits фолдах\n",
    "    X = df.drop('new_salary', axis=1, errors=\"ignore\")\n",
    "    y = df[\"new_salary\"]\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=7575)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Специальный класс для ускорения обучения\n",
    "        train_dataset = Pool(data=X_train, label=y_train, cat_features=['region_name'])\n",
    "        eval_dataset = Pool(data=X_test, label=y_test, cat_features=['region_name'])\n",
    "\n",
    "        model = CatBoostRegressor(cat_features=['region_name'],\n",
    "                                eval_metric = 'MAPE',\n",
    "                                depth=7,\n",
    "                                iterations=10000,\n",
    "                                early_stopping_rounds=100,\n",
    "                                random_state=7575,\n",
    "                                verbose=False\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        model.fit(train_dataset, \n",
    "                eval_set=eval_dataset,\n",
    "                verbose=False,\n",
    "                use_best_model=True,\n",
    "                early_stopping_rounds=150)\n",
    "\n",
    "        \n",
    "        regs.append(model)\n",
    "        scores.append(model.best_score_['validation']['MAPE'])\n",
    "\n",
    "    return scores, regs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Bundles = [116, 119, 128, 134, 16, 27, 283, 31, 33, 336, 49, 52, 54, 58, 61, 663, 69, 706, 74, 767, 783, 833, 836, 91, 919]\n",
    "\n",
    "for n_bundle in Bundles:\n",
    "    # try:\n",
    "    #     main_path_save = f'./predicted/{n_bundle}'\n",
    "    #     os.mkdir(main_path_save)\n",
    "    # except:\n",
    "    #     None\n",
    "\n",
    "\n",
    "    dt = pd.read_csv(f'./dfs_report/{n_bundle}.csv') # тестовый, смотрим где mape>=0.3\n",
    "    dt = dt[dt['salary_actual']>5000]\n",
    "    target_column = 'experience_id'\n",
    "    look_df = get_metric_rate(target_column, dt)\n",
    "    look_df_small_mape = look_df[look_df['mape']>=0.3]\n",
    "\n",
    "    # в словарь сохраняем все значения target_column при которых mape>=0.3\n",
    "    bad_exps = look_df_small_mape[target_column].values.tolist()\n",
    "    print(n_bundle, bad_exps)\n",
    "\n",
    "    if bad_exps == []:\n",
    "        continue\n",
    "    \n",
    "    continue\n",
    "\n",
    "\n",
    "    df, indexes = get_df(n_bundle, 'new_salary_quantile', 0)\n",
    "\n",
    "    for exp_id in bad_exps:\n",
    "        df = df[df['experience_id']==exp_id]\n",
    "        mapes, models = cb_for_bad(df)\n",
    "        print(f'для бундла {n_bundle}, опыт {exp_id}, мапе {np.mean(mapes)}')\n",
    "        # сохраняем модели\n",
    "        # создаем папку \n",
    "        try:\n",
    "            os.mkdir(f'{main_path_save}/exp{exp_id}')\n",
    "        except:\n",
    "            ...\n",
    "\n",
    "        for n, model in enumerate(regs):\n",
    "            model.save_model(f\"{temp_path}/exp{exp_id}/catboost_model_{n}.cbm\", format=\"cbm\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "4445208e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "get_metric_rate() missing 1 required positional argument: 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[340], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m df \u001b[39m=\u001b[39m df[df[\u001b[39m'\u001b[39m\u001b[39msalary_actual\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m5000\u001b[39m]\n\u001b[0;32m      5\u001b[0m target_column \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexperience_id\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> 6\u001b[0m look_df \u001b[39m=\u001b[39m get_metric_rate(target_column, df)\n\u001b[0;32m      7\u001b[0m look_df_small_mape \u001b[39m=\u001b[39m look_df[look_df[\u001b[39m'\u001b[39m\u001b[39mmape\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m>\u001b[39m\u001b[39m=\u001b[39m\u001b[39m0.3\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[39m# в словарь сохраняем все значения target_column при которых mape>=0.3\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: get_metric_rate() missing 1 required positional argument: 'df'"
     ]
    }
   ],
   "source": [
    "dict_bad = {}\n",
    "path = './dfs_report/283.csv'\n",
    "df = pd.read_csv(path)\n",
    "df = df[df['salary_actual']>5000]\n",
    "target_column = 'experience_id'\n",
    "look_df = get_metric_rate(target_column, df)\n",
    "look_df_small_mape = look_df[look_df['mape']>=0.3]\n",
    "# в словарь сохраняем все значения target_column при которых mape>=0.3\n",
    "dict_bad[target_column] = look_df_small_mape[target_column].values.tolist()\n",
    "look_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d290cdac",
   "metadata": {},
   "source": [
    "# мой кошачий буст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "bf4ec7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сейчас уже берем данные из изначальных бундлов\n",
    "n_bundle = 283\n",
    "path_to_train = './Bundles2023'\n",
    "path = './dfs_report/283.csv'\n",
    "\n",
    "df, indexes = get_df(283, 'new_salary_quantile', 0)\n",
    "df = df[df['experience_id']==0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "c8dc196b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cb_for_bad(df):\n",
    "\n",
    "    n_splits = 3  # Число фолдов\n",
    "    regs = [] # тут сохр модели\n",
    "    scores = [] # тут будем хранить скоры валидационных фолдов\n",
    "\n",
    "    # параметры валидации, обучение будет идти на n_splits фолдах\n",
    "    X = df.drop('new_salary', axis=1, errors=\"ignore\")\n",
    "    y = df[\"new_salary\"]\n",
    "\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=7575)\n",
    "    for train_index, test_index in kf.split(X):\n",
    "\n",
    "        X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "        # Специальный класс для ускорения обучения\n",
    "        train_dataset = Pool(data=X_train, label=y_train, cat_features=['region_name'])\n",
    "        eval_dataset = Pool(data=X_test, label=y_test, cat_features=['region_name'])\n",
    "\n",
    "        model = CatBoostRegressor(cat_features=['region_name'],\n",
    "                                eval_metric = 'MAPE',\n",
    "                                depth=7,\n",
    "                                iterations=10000,\n",
    "                                early_stopping_rounds=100,\n",
    "                                random_state=7575,\n",
    "                                verbose=False\n",
    "                                )\n",
    "        \n",
    "        \n",
    "        model.fit(train_dataset, \n",
    "                eval_set=eval_dataset,\n",
    "                verbose=False,\n",
    "                use_best_model=True,\n",
    "                early_stopping_rounds=150)\n",
    "\n",
    "        \n",
    "        regs.append(model)\n",
    "        scores.append(model.best_score_['validation']['MAPE'])\n",
    "\n",
    "    return scores, regs\n",
    "\n",
    "\n",
    "mapes, models = cb_for_bad(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "a0803505",
   "metadata": {},
   "outputs": [],
   "source": [
    "# сохраняем модели\n",
    "n_bundle = 283\n",
    "try:\n",
    "    temp_path = f'./predicted/{n_bundle}'\n",
    "    os.mkdir(temp_path)\n",
    "    os.mkdir(f'{temp_path}/exp0')\n",
    "\n",
    "except:\n",
    "    ...\n",
    "\n",
    "for n, model in enumerate(regs):\n",
    "    model.save_model(f\"{temp_path}/exp0/catboost_model_{n}.cbm\", format=\"cbm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "481d9760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d15ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12161369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # предсказание на тестовых данных\n",
    "# n_bundle = 16\n",
    "# df = pd.read_csv(f'{path_to_train}/{n_bundle}.csv')\n",
    "# coef = coef_dict[bundle]\n",
    "# salary = 'new_salary'\n",
    "# start_indixes = df.index.tolist()\n",
    "# test_y = np.array(df.new_salary) # изначальная зп\n",
    "# # salary_actual\n",
    "# salary_actual = df['new_salary'].values.tolist()\n",
    "# #Если надо загрузить уже имеющуюся модель\n",
    "# trained_model = json.load(open(f'{path_to_e_models}/{n_bundle}.json'))\n",
    "# trained_model = convert_keys_to_int(trained_model)\n",
    "\n",
    "# cb_models = []\n",
    "# for i in range(3):\n",
    "#     cr = CatBoostRegressor()\n",
    "#     cr.load_model(f'{path_to_cb_models}/{n_bundle}/catboost_model_{i}.cbm')\n",
    "#     cb_models += [cr]\n",
    "\n",
    "# columns_list = cb_models[0].feature_names_\n",
    "# df = df[cb_models[0].feature_names_]\n",
    "\n",
    "# #Получаем предсказания\n",
    "# cb_preds,mod_preds,actuals = get_predicts_both(df,trained_model,cb_models,test_y)\n",
    "# cb_preds_n = np.array(cb_preds).flatten()\n",
    "# mod_preds_n = np.array(mod_preds).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58563311",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e48dddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bad_Bundles = []\n",
    "# for n_bundle in Bundles:\n",
    "#     df = pd.read_csv(f'{path_to_train}/based_b_{n_bundle}.csv')\n",
    "#     if df.shape[0] < 1000:\n",
    "#         print(f'{n_bundle} | {df.shape}')\n",
    "#         bad_Bundles.append(n_bundle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74595772",
   "metadata": {},
   "source": [
    "## Делаем предсказания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "42cf4428",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bundles = [116, 119, 124, 128, 134, 16, 23, 27, 283, 31, 33, 336, 357, 43,49,52,54, 561, 58, 61, 663, 69, 693, 706, 712, 74, 742, \\\n",
    " 765, 767, 783, 811, 812, 833, 836, 84, 85, 852, 853, 856, 87, 873, 876, 888, 898, 91, 917, 919, 921, 954, 992]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "d3547516",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e97c990",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa486160",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "0a3cddb0",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'cb_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m salary \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39msalary_actual\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     13\u001b[0m \u001b[39m# for salary in ['salary_actual']:\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39msalary_predicted\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mcb_pred \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m coef) \u001b[39m+\u001b[39m df\u001b[39m.\u001b[39mmodel_pred \u001b[39m*\u001b[39m coef\n\u001b[0;32m     15\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mmore25\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(df\u001b[39m.\u001b[39msalary_predicted \u001b[39m-\u001b[39m df\u001b[39m.\u001b[39msalary_actual)\u001b[39m/\u001b[39mdf\u001b[39m.\u001b[39msalary_actual \u001b[39m>\u001b[39m \u001b[39m0.25\u001b[39m\n\u001b[0;32m     16\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39msalary_actual\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[salary]\n",
      "File \u001b[1;32mc:\\Users\\maria\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\generic.py:5989\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5982\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   5983\u001b[0m     name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_internal_names_set\n\u001b[0;32m   5984\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_metadata\n\u001b[0;32m   5985\u001b[0m     \u001b[39mand\u001b[39;00m name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accessors\n\u001b[0;32m   5986\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[0;32m   5987\u001b[0m ):\n\u001b[0;32m   5988\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m[name]\n\u001b[1;32m-> 5989\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mobject\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getattribute__\u001b[39;49m(\u001b[39mself\u001b[39;49m, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'cb_pred'"
     ]
    }
   ],
   "source": [
    "path_save_final = './predicted'\n",
    "\n",
    "for bundle in [16]:\n",
    "    # if 'csv' not in bundle:\n",
    "    #     continue\n",
    "    # print(bundle)\n",
    "    # bundle=int(bundle.replace('.csv',''))\n",
    "    df = pd.read_csv(f'{path_to_train}/based_b_{n_bundle}.csv')\n",
    "    \n",
    "    # for coef in np.linspace(0,1,11):\n",
    "    coef = coef_dict[bundle]\n",
    "    salary = 'salary_actual'\n",
    "    # for salary in ['salary_actual']:\n",
    "    df['salary_predicted'] = df.cb_pred * (1 - coef) + df.model_pred * coef\n",
    "    df['more25'] = abs(df.salary_predicted - df.salary_actual)/df.salary_actual > 0.25\n",
    "    df['salary_actual'] = df[salary]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb9b57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775820d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a78324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af28fdf8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c72767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "527bc808-2b17-4886-b2e1-6ac5ee849e8c",
   "metadata": {},
   "source": [
    "# для формирования файла results2308"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6c1a742e-2d7b-4b31-9809-9939b024832a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path_save_final = '/home/admin/skill_value/Bundles25_pred'\n",
    "charts_save = '/home/admin/skill_value/results2308'\n",
    "\n",
    "for bundle in [16]:\n",
    "    # if 'csv' not in bundle:\n",
    "    #     continue\n",
    "    # print(bundle)\n",
    "    # bundle=int(bundle.replace('.csv',''))\n",
    "    df = pd.read_csv(f'{path_save_final}/{bundle}.csv')\n",
    "    \n",
    "    # for coef in np.linspace(0,1,11):\n",
    "    coef = coef_dict[bundle]\n",
    "    salary = 'salary_actual'\n",
    "    # for salary in ['salary_actual']:\n",
    "    df['salary_predicted'] = df.cb_pred * (1 - coef) + df.model_pred * coef\n",
    "    df['more25'] = abs(df.salary_predicted - df.salary_actual)/df.salary_actual > 0.25\n",
    "    df['salary_actual'] = df[salary]\n",
    "    # break\n",
    "    try:\n",
    "        os.mkdir(f'{charts_save}/{bundle}')\n",
    "    except:\n",
    "        ...\n",
    "    try:\n",
    "        os.mkdir(f'{charts_save}/{bundle}/{salary}')\n",
    "    except:\n",
    "        ...\n",
    "\n",
    "    try:\n",
    "        os.mkdir(f'{charts_save}/{bundle}/{salary}/plots')\n",
    "    except:\n",
    "        ...\n",
    "    #  регион, график работы, вахта, зарплата, требования к опыту\n",
    "    for mode in ['more25', 'less25']:\n",
    "        try:\n",
    "            os.mkdir(f'{charts_save}/{bundle}/{salary}/{mode}')\n",
    "        except:\n",
    "            ...\n",
    "        #  смотрим в разрезе столбца more_25\n",
    "        if mode == 'more25':\n",
    "            temp_df = df[df['more25'] == True]\n",
    "            # 2) график распределения вакансий с высокой долей ошибок 25%+ по industry_group\n",
    "            save_path = f'{charts_save}/{bundle}/{salary}/{mode}/industry_group.png'\n",
    "            make_chart(temp_df, 'industry_group', save_path)\n",
    "\n",
    "\n",
    "        elif mode == 'less25':\n",
    "            temp_df = df[df['more25'] == False]\n",
    "\n",
    "            # 1) графики распределения характеристик: регион, график работы, вахта, зарплата, требования к опыту в разрезе 2 групп\n",
    "        for distrib in ['region_name', 'is_parttime', 'is_vahta', 'salary_actual', 'salary_predicted', 'experience_id']:\n",
    "            save_path = f'{charts_save}/{bundle}/{salary}/{mode}/{distrib}.png'\n",
    "            if distrib in ['salary_actual', 'salary_predicted']:\n",
    "                plot_only_salary2(temp_df, distrib, save_path)\n",
    "            else:\n",
    "                make_chart(temp_df, distrib, save_path)\n",
    "\n",
    "\n",
    "        # 3) сделать 2 распределения (настоящая зп и предсказанная) на одном графике\n",
    "        save_path = f'{charts_save}/{bundle}/{salary}/{mode}_salary.png'\n",
    "        plot_salary_distribution(temp_df, save_path,['Salary_distribution','salary','count'])\n",
    "\n",
    "        counter = 2\n",
    "    for tg_c in ['region_name', 'is_vahta','is_parttime', 'industry_group', 'experience_id','','year']:\n",
    "        \n",
    "        if tg_c == '':\n",
    "            counter += 1\n",
    "            continue\n",
    "        try:\n",
    "            os.mkdir(f'{charts_save}/{bundle}/{salary}/plots/{tg_c}_table_{counter}')\n",
    "        except:\n",
    "            ...\n",
    "        dm = get_metric_rate(tg_c,'salary_actual', df)\n",
    "        dm['mape'] = np.round(dm.mape, 3)\n",
    "        dm['rmse'] = np.round(dm.rmse, 3)\n",
    "        dm.to_csv(f'{charts_save}/{bundle}/{salary}/table_{counter}.csv')\n",
    "        for value in np.unique(df[tg_c]):\n",
    "            labels = [tg_c + '_salary_distribution','Salary','Count']\n",
    "            dt = df[df[tg_c] == value]\n",
    "            save_path = f'{charts_save}/{bundle}/{salary}/plots/{tg_c}_table_{counter}/{value}.png'\n",
    "            plot_salary_distribution(dt, save_path,labels)\n",
    "\n",
    "        counter+=1\n",
    "\n",
    "    d7 = get_7(df)\n",
    "    d7.to_csv(f'{charts_save}/{bundle}/{salary}/table_7.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "755f93e1-ee9e-440b-9e60-46f14dd71e4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c988b406-319e-4763-844b-a77afe880b7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dff6836-6cc8-4ba3-bde1-07bb79d196a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921773ce-ecbd-4897-913d-8f6cf291f2c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bf3863-9aa3-4f5c-943b-eabe994a6fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d01049b-efdf-4725-a7ef-abc30c732ece",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796a1a08-e245-48f8-ba7e-cf87eb22b372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785b417e-99fd-459a-afa4-75401fff40a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3d82d5-492f-4c4e-9ae9-fac65f89ff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6919ce7-5bc2-44ce-9224-f2e5ee1fd109",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb51d62-9266-449c-9aa0-967a932e26d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e1278e-39a0-4749-bd83-0fee5ccc19a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07478b64-a98e-4588-83ca-c4bfbfa9ec39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3bd0f16-b3c1-4bec-8155-07b768f7164e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c5c45b-60b7-4d94-a3fb-b5071c7861d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e89cb7-abec-4259-9323-b0fed21df2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "3086e5f2-f758-4a63-a98d-5d600201ba8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bundles = os.listdir('/home/admin/skill_value/results2308')\n",
    "# for n_bundle in bundles:\n",
    "#     if '.ip' in n_bundle : continue\n",
    "#     results = pd.DataFrame({}, columns = ['n_bundle','N','mape','rmse','is_vahta','experience_id','industry_group','is_parttime'])\n",
    "#     df = pd.read_csv(f'{path_save_final}/{n_bundle}.csv')\n",
    "    \n",
    "#     # for coef in np.linspace(0,1,11):\n",
    "#     coef = coef_dict[bundle]\n",
    "#     salary = 'salary_actual'\n",
    "#     # for salary in ['salary_actual']:\n",
    "#     df['salary_predicted'] = df.cb_pred * (1 - coef) + df.model_pred * coef\n",
    "#     df['more25'] = abs(df.salary_predicted - df.salary_actual)/df.salary_actual > 0.25\n",
    "#     df['salary_actual'] = df[salary]\n",
    "#     for is_vahta in [0,1]:\n",
    "#         for experience_id in [0,1,2]:\n",
    "#             for industry_group in [0,1,2]:\n",
    "#                 for is_parttime in [0,1]:\n",
    "#                     dt = df[(df['year'] == 2023) & \n",
    "#                             (df['experience_id'] == experience_id) &\n",
    "#                             (df['industry_group'] == industry_group) &\n",
    "#                             (df['is_parttime'] == is_parttime)&\n",
    "#                             (df['is_vahta'] == is_vahta)]\n",
    "#                     try:\n",
    "#                         mape_res = round(mape(dt.salary_actual, dt.salary_predicted),3)\n",
    "#                         rmse_res = round(rmse(dt.salary_actual, dt.salary_predicted),3)\n",
    "#                         metric_X, metric_Y, persent_df = validate_XY(dt[salary], dt['salary_predicted'])\n",
    "\n",
    "#                     except ValueError:\n",
    "#                         continue\n",
    "#                     if results.shape[1] < 10:\n",
    "#                         results = pd.concat([results, persent_df], axis=1)\n",
    "#                     results = results.dropna()\n",
    "#                     more25 = metric_Y['более 25%']\n",
    "#                     less25 = metric_X['не более 25%']\n",
    "                    \n",
    "#                     less25_arr = list(np.round(np.array(list(metric_X.values()))/(more25+less25),3))\n",
    "#                     more25_arr = list(np.round(np.array(list(metric_Y.values()))/(more25+less25),3))\n",
    "\n",
    "#                     results.loc[results.shape[0]] = [n_bundle, dt.shape[0], mape_res,rmse_res,is_vahta, experience_id, industry_group, is_parttime] + less25_arr + more25_arr\n",
    "    \n",
    "#     results = results.sort_values(by = 'mape')\n",
    "#     results.to_csv(f'/home/admin/skill_value/results2308_2023_analysis/{n_bundle}.csv',index = False)\n",
    "#     # break\n",
    "#     # print(f\"is_vahta {is_vahta}; experience_id {experience_id}; industry_group {industry_group}; is_parttime {is_parttime} \")\n",
    "#                     # print(f\"mape {round(mape(dt.salary_actual, dt.salary_predicted),3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1c759e79-4f47-40d1-b0c6-6637ce262c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for bundle in os.listdir(charts_save):\n",
    "#     if '.ipynb' in bundle: continue\n",
    "#     # try:\n",
    "#     df = pd.read_csv(f'{charts_save}/{bundle}/{salary}/table_7.csv')\n",
    "#     # df['Группа ЗП'] = df['Регион']\n",
    "#     # df= df.drop('Регион', axis = 1)\n",
    "#     df= df.drop('Unnamed: 0.1', axis = 1)\n",
    "#     df = df[['Группа ЗП'] + [x for x in list(df) if x != 'Группа ЗП']]\n",
    "#     df.to_csv(f'{charts_save}/{bundle}/{salary}/table_7.csv',index = False)\n",
    "#     # except:\n",
    "#     #     ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2baebd35-d300-44a6-bb4f-be5e3f1eff0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "348df07580ce3949ed2f2e4d542d2971039168d571eff187c4c4aba02e0840d0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
